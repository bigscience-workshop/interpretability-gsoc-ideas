# interpretability-gsoc-ideas
<img align="right" height="27%" width="27%" src="https://images.squarespace-cdn.com/content/v1/616206d72ac74d0c65656167/4aadf3be-fea0-489d-bfee-4f376e724eb7/sakura-flower-clipart-md-spaces.png?format=250w"/>
Welcome to our BigScience Interpretability Ideas repo for the GSoC 2022!

During the BigScience workshop, we've [established a community](https://bigscience.huggingface.co) of collaborators from all around the world. 
A specific group of the BigScience community developed a [vision](https://youtu.be/NL1_kMOkHm8?t=647) for the exact steps needed to improve the interpretability of NLP models. 
The implementation of these interpretability insights are the core idea of our Summer of Code! 
Our ideas are backed by the one-year-long research and are in line with the ongoing projects.

BigScience workshop is community-driven by design, and as the newcomer contributors you will join the community once you start working on proposals.
During the GSoC, we expect contributors to follow the community groups' agenda and attend the sync calls together with all the group contributors.

Do not hesitate to **reach out to us on the [bigscience-gsoc@googlegroups.com](bigscience-gsoc@googlegroups.com)** with general questions, and to **discuss and propose the [Ideas](https://github.com/bigscience-workshop/interpretability-gsoc-ideas/issues)** in the [Issues](https://github.com/bigscience-workshop/interpretability-gsoc-ideas/issues) threads!

